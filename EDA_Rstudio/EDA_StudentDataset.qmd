---
title: "Data Programming Project 2024"
author: "Lucas Lorenzo Jakin, 89211034"
data: "2024-06-04"
output: html_document
editor: visual
format: 
  html: 
    toc: true
    toc-location: right
    embed-resources: true
bibliography: references.bib
execute: 
  echo: true
  error: true
  warning: false
  messages: false
---

# Predicting Secondary School Student Performance (EDA)

## Student Performance Dataset

Education is a key factor for achieving a long-term economic progress. During the last decades, the portuguese educational level has improved. However, the statistics keep Portugal at Europe's tail end due to its *high student failure* and *dropping rates*.

In particular, failure in the core classes of Mathematics and Portuguese (native language) is extremely serious, since they provide fundamental knowledge for the success in other remaining subjects.

The **chosen dataset** (@kaggle_dataset_StudPerformance) contains real-world data from two Portuguese secondary schools. Data was collected using two sources: mark reports, and questionnaires.

------------------------------------------------------------------------

```{r}
library(tidyverse)
library(dplyr)
library(skimr)
library(ggcorrplot)
library(gt)
library(ggplot2)
dataset <- read.csv("student_data.csv", header = T)
nrow(dataset)
```

Now about the dataframe; The initial version contained scarce information (only the grades and number of absences were available), it was complemented with the latter, which allowed the collection of several demographic, social and school related attributes. The final dataframe consists of 33 columns (attributes), where G3 represents the target variable, the one that we want to predict.

The dataset looks like the following:

```{r}
  head(dataset)
  
```

------------------------------------------------------------------------

\

# Attributes & their descriptions

> Data Distribution

```{r}
  skim(dataset)
```

\

-   In this analysis I used the **skim()** function that provides a neat summary of the dataset, revealing key statistics and characteristics of the data. The dataframe is "partitioned" into two distinct data types: **Numerical** and **String**.
-   Numerical attributes were examined for statistical measures(mean,median,standard deviation) and gave a description on how are the attributes distributed.
-   The dataframe does not contain any missing values, which is crucial for the modeling(algorithm) part and making predictions.

```{r}
library(knitr)
att <- colnames(dataset)
description <- c("student's school(binary)","student's sex(binary)" ,"student's age(Num)","home address","family size(binary)" ,"parent's cohabition status(binary)","mother's education(Num)","father's education(Num)","mother's job(nominal)","father's job(nominal)","reason to choose school(nominal)","student's guardian(nominal)","travel time (Num)","weekly study time(Num)","past class failures(Num)","extra school support(binary)","family educational support(binary)","extra paid classes(binary)","extra-curricular activities(binary)","attended nursery school(binary)","wants to take higher education(binary)","internet access(binary)","romantic relationship(binary)","quality of family relationship(Num)","free time(Num)","going out w/ friends(Num)","workday alcohol consumption(Num)","weekend alcohol consumption(Num)","health status(Num)","absences (Num)","first period grade(Num)","second period grade(Num)","final grade(Num)")

table<- data.frame(Attribute = att, Description = description)

```

@tbl-attributes listed below shows all the columns that are present inside the data frame combined with their descriptions and data type.

```{r}
#| label: tbl-attributes
#| tbl-cap: "The preprocessed student related variables"
  table %>% gt()

```

## String Data Distributions

> SCHOOL

```{r}
dataset %>% select(school) %>%
  count(school) %>%
  ggplot(., aes(x="", n, fill = school)) +
  geom_bar(width = 1, size = 1, color = "white",stat = "identity") + coord_polar("y", start = 0) +
  geom_text(aes(label = paste0(round((n/395)*100),"%")),
            position = position_stack(vjust = 0.5)) +
  theme_classic() +
  labs(x = NULL, y = NULL) +
  theme(axis.line = element_blank())

```

> SEX

```{r}
dataset %>% select(sex) %>%
  count(sex) %>%
  ggplot(., aes(x="", n, fill = sex)) +
  geom_bar(width = 1, size = 1, color = "white",stat = "identity") + coord_polar("y", start = 0) +
  geom_text(aes(label = paste0(round((n/395)*100),"%")),
            position = position_stack(vjust = 0.5)) +
  theme_classic() +
  labs(x = NULL, y = NULL) +
  theme(axis.line = element_blank())
```

> ADDRESS

```{r}
dataset %>% select(address) %>%
  count(address) %>%
  ggplot(., aes(x="", n, fill = address)) +
  geom_bar(width = 1, size = 1, color = "white",stat = "identity") + coord_polar("y", start = 0) +
  geom_text(aes(label = paste0(round((n/395)*100),"%")),
            position = position_stack(vjust = 0.5)) +
  theme_classic() +
  labs(x = NULL, y = NULL) +
  theme(axis.line = element_blank())
```

> FAMSIZE

```{r}
dataset %>% select(famsize) %>%
  count(famsize) %>%
  ggplot(., aes(x="", n, fill = famsize)) +
  geom_bar(width = 1, size = 1, color = "white",stat = "identity") + coord_polar("y", start = 0) +
  geom_text(aes(label = paste0(round((n/395)*100),"%")),
            position = position_stack(vjust = 0.5)) +
  theme_classic() +
  labs(x = NULL, y = NULL) +
  theme(axis.line = element_blank())
```

> PSTATUS

```{r}
dataset %>% select(Pstatus) %>%
  count(Pstatus) %>%
  ggplot(., aes(x="", n, fill = Pstatus)) +
  geom_bar(width = 1, size = 1, color = "white",stat = "identity") + coord_polar("y", start = 0) +
  geom_text(aes(label = paste0(round((n/395)*100),"%")),
            position = position_stack(vjust = 0.5)) +
  theme_classic() +
  labs(x = NULL, y = NULL) +
  theme(axis.line = element_blank())
```

> MJOB

```{r}
dataset %>% select(Mjob) %>%
  count(Mjob) %>%
  ggplot(., aes(x="", n, fill = Mjob)) +
  geom_bar(width = 1, size = 1, color = "white",stat = "identity") + coord_polar("y", start = 0) +
  geom_text(aes(label = paste0(round((n/395)*100),"%")),
            position = position_stack(vjust = 0.5)) +
  theme_classic() +
  labs(x = NULL, y = NULL) +
  theme(axis.line = element_blank())
```

> FJOB

```{r}
dataset %>% select(Fjob) %>%
  count(Fjob) %>%
  ggplot(., aes(x="", n, fill = Fjob)) +
  geom_bar(width = 1, size = 1, color = "white",stat = "identity") + coord_polar("y", start = 0) +
  geom_text(aes(label = paste0(round((n/395)*100),"%")),
            position = position_stack(vjust = 0.5)) +
  theme_classic() +
  labs(x = NULL, y = NULL) +
  theme(axis.line = element_blank())
```

> REASON

```{r}
dataset %>% select(reason) %>%
  count(reason) %>%
  ggplot(., aes(x="", n, fill = reason)) +
  geom_bar(width = 1, size = 1, color = "white",stat = "identity") + coord_polar("y", start = 0) +
  geom_text(aes(label = paste0(round((n/395)*100),"%")),
            position = position_stack(vjust = 0.5)) +
  theme_classic() +
  labs(x = NULL, y = NULL) +
  theme(axis.line = element_blank())
```

> GUARDIAN

```{r}
dataset %>% select(guardian) %>%
  count(guardian) %>%
  ggplot(., aes(x="", n, fill = guardian)) +
  geom_bar(width = 1, size = 1, color = "white",stat = "identity") + coord_polar("y", start = 0) +
  geom_text(aes(label = paste0(round((n/395)*100),"%")),
            position = position_stack(vjust = 0.5)) +
  theme_classic() +
  labs(x = NULL, y = NULL) +
  theme(axis.line = element_blank())
```

> SCHOOLSUP

```{r}
dataset %>% select(schoolsup) %>%
  count(schoolsup) %>%
  ggplot(., aes(x="", n, fill = schoolsup)) +
  geom_bar(width = 1, size = 1, color = "white",stat = "identity") + coord_polar("y", start = 0) +
  geom_text(aes(label = paste0(round((n/395)*100),"%")),
            position = position_stack(vjust = 0.5)) +
  theme_classic() +
  labs(x = NULL, y = NULL) +
  theme(axis.line = element_blank())
```

> FAMSUP

```{r}
dataset %>% select(famsup) %>%
  count(famsup) %>%
  ggplot(., aes(x="", n, fill = famsup)) +
  geom_bar(width = 1, size = 1, color = "white",stat = "identity") + coord_polar("y", start = 0) +
  geom_text(aes(label = paste0(round((n/395)*100),"%")),
            position = position_stack(vjust = 0.5)) +
  theme_classic() +
  labs(x = NULL, y = NULL) +
  theme(axis.line = element_blank())
```

> PAID

```{r}
dataset %>% select(paid) %>%
  count(paid) %>%
  ggplot(., aes(x="", n, fill = paid)) +
  geom_bar(width = 1, size = 1, color = "white",stat = "identity") + coord_polar("y", start = 0) +
  geom_text(aes(label = paste0(round((n/395)*100),"%")),
            position = position_stack(vjust = 0.5)) +
  theme_classic() +
  labs(x = NULL, y = NULL) +
  theme(axis.line = element_blank())
```

> ACTIVITIES

```{r}
dataset %>% select(activities) %>%
  count(activities) %>%
  ggplot(., aes(x="", n, fill = activities)) +
  geom_bar(width = 1, size = 1, color = "white",stat = "identity") + coord_polar("y", start = 0) +
  geom_text(aes(label = paste0(round((n/395)*100),"%")),
            position = position_stack(vjust = 0.5)) +
  theme_classic() +
  labs(x = NULL, y = NULL) +
  theme(axis.line = element_blank())
```

> NURSERY

```{r}
dataset %>% select(nursery) %>%
  count(nursery) %>%
  ggplot(., aes(x="", n, fill = nursery)) +
  geom_bar(width = 1, size = 1, color = "white",stat = "identity") + coord_polar("y", start = 0) +
  geom_text(aes(label = paste0(round((n/395)*100),"%")),
            position = position_stack(vjust = 0.5)) +
  theme_classic() +
  labs(x = NULL, y = NULL) +
  theme(axis.line = element_blank())
```

> HIGHER

```{r}
dataset %>% select(higher) %>%
  count(higher) %>%
  ggplot(., aes(x="", n, fill = higher)) +
  geom_bar(width = 1, size = 1, color = "white",stat = "identity") + coord_polar("y", start = 0) +
  geom_text(aes(label = paste0(round((n/395)*100),"%")),
            position = position_stack(vjust = 0.5)) +
  theme_classic() +
  labs(x = NULL, y = NULL) +
  theme(axis.line = element_blank())
```

> INTERNET

```{r}
dataset %>% select(internet) %>%
  count(internet) %>%
  ggplot(., aes(x="", n, fill = internet)) +
  geom_bar(width = 1, size = 1, color = "white",stat = "identity") + coord_polar("y", start = 0) +
  geom_text(aes(label = paste0(round((n/395)*100),"%")),
            position = position_stack(vjust = 0.5)) +
  theme_classic() +
  labs(x = NULL, y = NULL) +
  theme(axis.line = element_blank())
```

> ROMANTIC

```{r}
dataset %>% select(romantic) %>%
  count(romantic) %>%
  ggplot(., aes(x="", n, fill = romantic)) +
  geom_bar(width = 1, size = 1, color = "white",stat = "identity") + coord_polar("y", start = 0) +
  geom_text(aes(label = paste0(round((n/395)*100),"%")),
            position = position_stack(vjust = 0.5)) +
  theme_classic() +
  labs(x = NULL, y = NULL) +
  theme(axis.line = element_blank())
```

------------------------------------------------------------------------

## Numeric Data Distributions

> DALC

```{r}
  boxplot(dataset$Dalc, col = "blue", border = "black")
```

> FEDU

```{r}
  boxplot(dataset$Fedu, col = "blue", border = "black")
```

> G1

```{r}
  boxplot(dataset$G1, col = "blue", border = "black")
```

> G2

```{r}
  boxplot(dataset$G2, col = "blue", border = "black")
```

> G3

```{r}
  boxplot(dataset$G3, col = "blue", border = "black")
```

> MEDU

```{r}
  boxplot(dataset$Medu, col = "blue", border = "black")
```

> WALC

```{r}
  boxplot(dataset$Walc, col = "blue", border = "black")
```

> ABSENCIES

```{r}
  boxplot(dataset$absences, col = "blue", border = "black")
```

> AGE

```{r}
  boxplot(dataset$age, col = "blue", border = "black")
```

> FAILURES

```{r}
  boxplot(dataset$failures, col = "blue", border = "black")
```

-   Pie chart

```{r}
dataset %>% select(failures) %>%
  count(failures) %>%
  ggplot(., aes(x="", n, fill = failures)) +
  geom_bar(width = 1, size = 1, color = "white",stat = "identity") + coord_polar("y", start = 0) +
  geom_text(aes(label = paste0(round((n/395)*100),"%")),
            position = position_stack(vjust = 0.5)) +
  theme_classic() +
  labs(x = NULL, y = NULL) +
  theme(axis.line = element_blank())
```

> FAMREL

```{r}
  boxplot(dataset$famrel, col = "blue", border = "black")
```

> FREETIME

```{r}
  boxplot(dataset$freetime, col = "blue", border = "black")
```

> GOOUT

```{r}
  boxplot(dataset$goout, col = "blue", border = "black")
```

> HEALTH

```{r}
  boxplot(dataset$health, col = "blue", border = "black")
```

> STUDYTIME

```{r}
  boxplot(dataset$studytime, col = "blue", border = "black")
```

> TRAVELTIME

```{r}
  boxplot(dataset$traveltime, col = "blue", border = "black")
```

------------------------------------------------------------------------

## HEATMAP of all attributes

```{r}
datasetCorr <- read.csv("CorrelationStudent.csv", header = T)
datasetCorr %>% model.matrix(~0+.,
              data=.) %>%
    cor(use="pairwise.complete.obs") %>% 
    ggcorrplot(show.diag = FALSE, type="full",
               lab=TRUE,legend.title = "Correlation" ,lab_size
               = 1.2,lab_col = "black" ,ggtheme =
                 ggplot2::theme_gray,
               colors = c("blue","white","red"),
               outline.color = "black")
```

------------------------------------------------------------------------

# Exploratary Data Analysis (EDA)

Classification and regression are two important DM goals. Both require a supervised learning, but they are different in terms of the output representation(descrete for classification and continuous for regression). In this work, the Mathematics and Poruguese grades (i.e. G3 of @tbl-attributes) will be modeled using three supervised approches:

-   **Binary** Classification: *pass* if G3 \geq 10, else *fail;*

-   **5-Level** Classification: based on *Erasmus* grade conversion system;

-   **Regression** of the G3 value : Numeric output between 0 and 20.

------------------------------------------------------------------------

Now let's look at key insights and patterns within the dataset. The purpose of this analysis is to understand the underlying patterns, distributions, and relationships within the data and to visualize the data through scatter plots, histograms and other graphical methods. This initial analysis will not only serve the requirements of the course but will also lay the groundwork of my final thesis. The findings will then provide a robust foundation for any subsequent analysis or modeling efforts.

------------------------------------------------------------------------

> Correlations between G1, G2

```{r}
#| label: fig-g1ng2
#| fig-cap: "Correlation - G1 & G2"
  dataset %>% select(G1,G2) %>% 
    ggplot(.,aes(G1,G2)) +
    geom_point(color = "darkblue", size = 3) +
    labs(x = "G1", y = "G2") +
    geom_smooth(color = "darkblue", method = "lm")
  
```

> Correlations between G2 & G3

```{r}
#| label: fig-g2ng3
#| fig-cap: "Correlation - G2 & G3"
  dataset %>% select(G2,G3) %>% 
    ggplot(.,aes(G2,G3)) +
    geom_point(color = "red", size = 3) +
    labs(x = "G2", y = "G3") +
    geom_smooth(color = "red", method = "lm")
  
```

> Correlations between G1 & G3

```{r}
#| label: fig-g1ng3
#| fig-cap: "Correlation G1 & G3"

dataset %>% select(G1,G3) %>% 
  ggplot(.,aes(G1,G3)) +
  geom_point(color = "darkgreen", size = 3) +
  labs(x = "G1", y = "G3") +
  geom_smooth(color = "darkgreen", method = "lm")
```

-   From the above three plots, we can see that these three attributes are correlated to each other. G1 and G2 represent the grade received from the first period and second school period, respectively. G3 is the target attribute and as we can see from the plots shown above it has a strong correlation with attributes G1 & G2. This occurs because G3 is the final year grade and this actually depends on the grades received from the first and second period.

------------------------------------------------------------------------

## Target attribute G3

-   After performing some analysis, I have decided to leave out the variables G1 and G2 and to continue with just having the target feature G3. I made this choice, because G1, G2 and G3 are very correlated to each other and G3 values are very similar to those from G1 and G2.

> 5 - Level Classification

This may be one possibility when trying to predict the final grade from students. G3 can be discretized into a separate columns that contains 5 different values/ classes, that represent the grading system used in Ireland. The values of attribute G3 go from 0 to 20, these can be discretized in the following way:

-   16-20 --\> class **A**

-   14-15 --\> class **B**

-   12-13 --\> class **C**

-   10-11 --\> class **D**

-   0-9 --\> class **F**

```{r}
datasetCategory <- dataset %>% mutate(Category = case_when(
  G3 >= 16 & G3 <= 20  ~ 'A',
  G3 >= 14 & G3 <= 15  ~ 'B',
  G3 >= 12 & G3 <= 13  ~ 'C',
  G3 >= 10 & G3 <= 11  ~ 'D',
  G3 >= 0 & G3 <= 9  ~ 'F',
  TRUE ~ NA_character_
))

datasetDiscretized <- datasetCategory %>% select(-G1, -G2, -G3)
#datasetDiscretized <- datasetCategory %>% select(-G3)
```

```{r}
datasetDiscretized %>% select(Category) %>%
  count(Category) %>%
  ggplot(., aes(Category, n, fill = Category)) +
  geom_col(width = 0.5)+
  labs(x = "Grades", y = "Count")+
  theme_minimal()

write.csv(datasetDiscretized,"discretizedDF.csv", row.names = FALSE)
colnames(datasetDiscretized)
```

```{r}
datasetDiscretized %>% select(Category) %>%
  count(Category)
```

> Regression

```{r}
dataset %>% select(G3) %>%
  count(G3) %>%
  ggplot(., aes(G3, n)) +
  geom_col(width = 0.5)+
  labs(x = "G3", y = "Count")+
  scale_x_continuous(breaks = seq(0,2, by = 5)) +
  theme_minimal()
```

Now we'll see how are the relationships between the target attribute G3 with other attributes.

## Age vs. Grade

### Age Distribution

```{r}
#| label: fig-ageD
#| fig-cap: "Age distribution"
dataset %>% select(age,G3) %>%
  count(age) %>%
  ggplot(., aes(age, n)) +
  geom_col(width = 0.5)+
  labs(x = "age", y = "Count")+
  scale_x_continuous(breaks = seq(15,22, by = 1)) +
  theme_minimal()+
  geom_smooth(color = "black", span=0.3,se = F)

```

-   @fig-ageD shows the distributions of the age of all students collected from the set of data. Analyzing the plot we can see that the majority of students fall within the range of **15 to 18**.

```{r}
#| label: tbl-ages
#| tbl-cap: "Mean grade values for each student's age"
  dataset %>% group_by(age) %>%
    summarize(
      G1_mean = mean(G1),
      G2_mean = mean(G2),
      G3_mean = mean(G3)
    ) %>% kable()
 
```

```{r}
dataset_long <- dataset %>%  
  pivot_longer(cols = c(G1,G2,G3), names_to = "grades",
               values_to = "value")
```

```{r}
#| label: fig-ageGs
#| fig-cap: "Mean of grades for all age distributions"
  ggplot(dataset_long,aes(age, mean(value), 
                        fill = as.factor(age))) +
    geom_col(width = 0.8) +
    facet_wrap(~grades, scales = "free_y") +
    labs(x = "Age", y = "Mean", fill = "Age") +
    scale_x_continuous(breaks = seq(15,22, by = 1)) 
  

```

-   From @tbl-ages and @fig-ageGs, we can conclude that the older a person is, the lower the grades they receive. However, **twenty-year olds** exhibit an outstanding performance (in this dataset): they **lead by at least 2** whole grade points on a 19-point scale.

\

## Job vs. Grade

> Mother's Job

```{r}
#| label: tbl-mJob
#| tbl-cap: "Mean grade values depending on mother's job"
  dataset %>% group_by(MotherJob = Mjob) %>%
    summarize(
      G3 = mean(G3)
    ) %>% kable()
```

```{r}
#| label: fig-mJobBar
#| fig-cap: "Mean grade values depending on mother's job"
  dataset %>% select(Mjob,G3) %>%
    group_by(Mjob) %>% summarize(mG3 = mean(G3)) %>% 
    ggplot(., aes(Mjob, mG3, fill = as.factor(Mjob))) +
    geom_bar(width = 0.5, stat = "identity")+
    labs(title="Job vs. Grade (Mother)",x = "Job", y = "G3", fill = "Mother's job")+
  theme_minimal()
```

-   Surprisingly, students with mothers who are **health care professionals** receive a higher grade on average. I personally expected students with mothers who were teachers to score the highest.

> Father's Job

```{r}
#| label: tbl-fJob
#| tbl-cap: "Mean grade values depending on father's job"
  dataset %>% group_by(FatherJob = Fjob) %>%
    summarize(
      G3 = mean(G3)
    ) %>% kable()
```

```{r}
#| label: fig-fJob
#| fig-cap: "Mean grade values depending on father's job"
  dataset %>% select(Fjob,G3) %>%
    group_by(Fjob) %>% summarize(mG3 = mean(G3)) %>%
    ggplot(., aes(Fjob, mG3, fill = as.factor(Fjob))) + 
    geom_bar(width = 0.5, stat = "identity")+ 
    labs(title="Job vs. Grade (Father)",x = "Job", y = "G3", fill = "Father's job")+
  theme_minimal()
```

-   Fathers with higher education levels tend to increase their children's performance more than fathers with lower education levels. This can be seen by the fact that students with fathers who are **health care professionals** or **teachers** receive a higher grade on average.

-   The father directly passes "forward" his knowledge, thus cultivating the student's knowledge

## Absences vs. Grade

```{r}
#| label: fig-absG3
#| fig-cap: "Correlation - absences & G3"
  dataset %>% select(absences,G3) %>% 
    ggplot(.,aes(absences,G3)) +
    geom_point(color = "black", size = 3) +
    labs(x = "absences", y = "G3") +
    geom_smooth(color = "black", method = "lm")
```

-   This low correlation could occur, because absent students "usually" revise the material, effectively accounting for their absence. For a further explanation, we need to plot the **correlation matrix** to validate the hypothesis.

```{r}
#| label: fig-absences
#| fig-cap: "Heatmap - absences,G1,G2,G3"
  dataset %>% select(absences,G1,G2,G3) %>% model.matrix(~0+.,
              data=.) %>%
    cor(use="pairwise.complete.obs") %>% 
    ggcorrplot(show.diag = FALSE, type="full",
               lab=TRUE,legend.title = "Correlation" ,lab_size
               = 4,lab_col = "black" ,ggtheme =
                 ggplot2::theme_gray,
               colors = c("black","red","white"),
               outline.color = "black")
  
```

-   The number of absences a student has, does not necessarily result in a lower grade overall, as absent student typically make up material they missed, effectively accounting for their absence.

# Preprocessing

------------------------------------------------------------------------

# METHODOLOGIES

-   **OneR model - One Rule**

    -   OneRClassifier (python) expects **categorical** or **discretized** features.

    -   Ensuring that features are categorical is **responsibility of user**

-   **MULTICLASS CLASSIFICATION -** Grading on a scale from A-F

    -   Decision Trees

    -   Naive Bayes

-   **REGRESSION -** Predicting exact numerical value based on inputs

    -   Linear Regression

    -   Lasso Regression

    -   Support Vector Machines

    -   KNN Model

    -   Decision Tree Regression

    -   Random Forest Regression

-   **CROSS VALIDATION - STRATIFIED**

    -   10 Fold Cross Valdiation

# MODELING

Let's set up Python in Rstudio:

```{r}
# SETTING UP PYTHON ON RSTUDIO
library(reticulate)

#virtualenv_create("my-python2", python_version = "3.12")

use_virtualenv("my-python2", required = TRUE)

#virtualenv_install(envname = "my-python2", "matplotlib",ignore_installed = FALSE, pip_options = character())

#virtualenv_install(envname = "my-python2", "catboost",ignore_installed = FALSE, pip_options = character())

#virtualenv_install(envname = "my-python2", "numpy",ignore_installed = FALSE, pip_options = character())

#virtualenv_install(envname = "my-python2", "pandas",ignore_installed = FALSE, pip_options = character())

#virtualenv_install(envname = "my-python2", "seaborn",ignore_installed = FALSE, pip_options = character())

#virtualenv_install(envname = "my-python2", "scikit-learn",ignore_installed = FALSE, pip_options = character())

#virtualenv_install(envname = "my-python2", "tensorflow",ignore_installed = FALSE, pip_options = character())
```

## HeatMap of attributes (above made in R) - just encoding dataset

```{python}
import pandas as pd
import numpy as np  
import seaborn as sns
import matplotlib.pyplot as plt
import sklearn.ensemble as es
from sklearn import metrics
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import KFold
from sklearn.preprocessing import StandardScaler, OneHotEncoder
#classifiers
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
# regressors
from sklearn.tree import DecisionTreeRegressor 
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor
#from sklearn.ensemble import RandomForestRegressor
df = pd.read_csv('student_data.csv')

#df['AvgGrade'] = (df['G1'] + df['G2'] + df['G3'])/3


le = LabelEncoder()
for column in df.columns:
    if df[column].dtype == 'object':
        df[column] = le.fit_transform(df[column])
        
df.info()

df.to_csv("CorrelationStudent.csv", index=False)
```

## OUTLIERS


## CLASSIFICATION

### ZeroR

```{r}

library(OneR)

# begin functions
ZeroR <- function(x, ...) {
  output <- OneR(cbind(dummy = TRUE, x[ncol(x)]), ...)
  class(output) <- c("ZeroR", "OneR")
  output
}

predict.ZeroR <- function(object, newdata, ...) {
  class(object) <- "OneR"
  predict(object, cbind(dummy = TRUE, newdata[ ncol(newdata)]), ...)
}

eval_model <- function(prediction, actual){
  confusion_matrix <- table(Predicted = prediction, Actual = actual$categoryClass)
  accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
  list(confusion_matrix = confusion_matrix, accuracy = accuracy)
}
# end functions
datasetDisc <- read.csv("discretizedDF.csv", header = TRUE)
head(datasetDisc)
datasetDisc$categoryClass <- factor(datasetDisc[, 31])
datasetDisc <- datasetDisc[, -31]
colnames(datasetDisc)
table(datasetDisc$categoryClass)

set.seed(12)
random <- sample(1:nrow(datasetDisc), 0.8 * nrow(datasetDisc))

#Split into TRAINING & TEST
#data_train <- datasetDisc[random, ]
data_train <- optbin(datasetDisc[random, ], method = "infogain")
data_test <- datasetDisc[-random, ]

model <- ZeroR(datasetDisc)
summary(model)

prediction <- predict(model, datasetDisc)

resultsZero <- eval_model(prediction, datasetDisc)
print(resultsZero$confusion_matrix)
print(paste("ZeroR accuracy is:", resultsZero$accuracy))
zeroRfinal <- (resultsZero$accuracy )
#modelOne <- OneR(categoryClass ~ ., data = data_train)
```

### OneR checkk!!

```{r}
library(OneR)

modelOne <- OneR(data_train, verbose = TRUE)

summary(modelOne)

prediction <- predict(modelOne, data_test)

resultsOne <- eval_model(prediction, data_test)

print(resultsOne$confusion_matrix)
print(paste("OneR accuracy is: ", resultsOne$accuracy))
oneRfinal <- (resultsOne$accuracy)

```

### Random Forests

```{python}
#| eval: false
data = pd.read_csv('discretizedDF.csv')
countR = 0
#Categorical features
cat_features = [feature for feature in data.columns if data[feature].dtype == 'O']

# Numerical attributes
num_features = [feature for feature in data.columns if data[feature].dtype != 'O']



le = LabelEncoder()
for column in cat_features:
    if data[column].dtype == 'object':
        data[column] = le.fit_transform(data[column])

X = data.drop("Category", axis=1)
y = data["Category"]

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=42)

modelR = es.RandomForestClassifier()
kfR = KFold(shuffle=True, n_splits=5)
RandForCvResults = cross_val_score(modelR, X, y, cv=kfR, scoring='accuracy')

RandForCvResults
print(RandForCvResults.mean())

tmp = modelR.fit(X_train,y_train)
tmp.score(X_test, y_test)
y_test_pred = modelR.predict(X_test)
for a,b in zip(y_test,y_test_pred):
    if(a==b):
        countR += 1
print("\n", classification_report(y_test,y_test_pred))
accRandFor = (countR/len(y_test)) * 100
print("\nAccuracy using Random Forest: ", str(round(accRandFor, 3)))
```

### K_Neighbors

```{python}
#| eval: false

KNclassifier = KNeighborsClassifier(n_neighbors=20)

dataN = pd.read_csv('discretizedDF.csv')

#Categorical features
cat_features = [feature for feature in dataN.columns if dataN[feature].dtype == 'O']

# Numerical attributes
num_features = [feature for feature in dataN.columns if dataN[feature].dtype != 'O']



le = LabelEncoder()
for column in cat_features:
    if dataN[column].dtype == 'object':
        dataN[column] = le.fit_transform(dataN[column])

X = data.drop("Category", axis=1)
y = data["Category"]

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=42)

kfK = KFold(shuffle=True, n_splits=5)
KnnCvResults = cross_val_score(KNclassifier, X, y, cv=kfK, scoring='accuracy')

KnnCvResults
print(KnnCvResults.mean())

tmp1 = KNclassifier.fit(X_train, y_train)
tmp1.score(X_test,y_test)
y_pred = KNclassifier.predict(X_test)

print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

KNAcc = accuracy_score(y_pred,y_test)
print('K Neighbours accuracy is: {:.2f}%'.format(KNAcc*100))
```


### SVM

```{python}
#| eval: false


dataS = pd.read_csv('discretizedDF.csv')

#Categorical features
cat_features = [feature for feature in dataS.columns if dataS[feature].dtype == 'O']

# Numerical attributes
num_features = [feature for feature in dataS.columns if dataS[feature].dtype != 'O']



le = LabelEncoder()
for column in cat_features:
    if dataS[column].dtype == 'object':
        dataS[column] = le.fit_transform(dataS[column])

X = data.drop("Category", axis=1)
y = data["Category"]

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=42)

SVCclassifier = SVC(kernel='linear', max_iter=251)
kfS = KFold(shuffle=True, n_splits=5)
SVCcvResults = cross_val_score(SVCclassifier, X, y, cv=kfS, scoring='accuracy')

SVCcvResults

print(SVCcvResults.mean())

tmp2 = SVCclassifier.fit(X_train, y_train)
tmp2.score(X_test,y_test)

y_pred = SVCclassifier.predict(X_test)

print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

SVCAcc = accuracy_score(y_pred,y_test)
print('SVC accuracy is: {:.2f}%'.format(SVCAcc*100))
```

## Regression

```{python}
dataReg = pd.read_csv('student_data.csv')

# Categorical attributes
cat_features = [feature for feature in dataReg.columns if dataReg[feature].dtype == 'O']

# Numerical attributes
num_features = [feature for feature in dataReg.columns if dataReg[feature].dtype != 'O']

# Encoding Categorical values
le = LabelEncoder()
for column in dataReg.columns:
    if dataReg[column].dtype == 'object':
        dataReg[column] = le.fit_transform(dataReg[column])

X = dataReg.drop("G3", axis = 1).values
y = dataReg["G3"].values

# Train Test Splitting
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)


```

### Function for regression metrics

```{python}
def Regression_metrics (model,X_train,y_train,X_test,y_test,y_pred):
    kf = KFold(shuffle=True, n_splits=5)
    cv_score = cross_val_score(estimator = model, X = X_train, y = y_train, cv = kf)
    
    # Calculating Adjusted R-squared
    r2 = model.score(X_test, y_test)
    # Number of observations is the shape along axis 0
    n = X_test.shape[0]
    # Number of features (predictors, p) is the shape along axis 1
    p = X_test.shape[1]
    # Adjusted R-squared formula
    adjusted_r2 = 1-(1-r2)*(n-1)/(n-p-1)
    RMSE = np.sqrt(metrics.mean_squared_error(y_test, y_pred))
    R2 = model.score(X_test, y_test)
    CV_R2 = cv_score.mean()
    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    return R2, adjusted_r2, CV_R2, RMSE, mae, mse
    
    print('MAE:',round(mae,4))
    print('MSE:',round(mse,4))
    print('RMSE:', round(RMSE,4))
    print('R2:', round(R2,4))
    print('Adjusted R2:', round(adjusted_r2, 4) )
    print("Cross Validated R2: ", round(cv_score.mean(),4) )
```


### Decision Tree Regressor

```{python}
DecisionTree_reg = DecisionTreeRegressor(random_state = 42)
DecisionTree_reg.fit(X_train, y_train)
# Making prediction on data
y_pred = DecisionTree_reg.predict(X_test)
  
ndf = [Regression_metrics(DecisionTree_reg, X_train, y_train, X_test, y_test, y_pred)]
  
dtScore = pd.DataFrame(data = ndf, columns=['R2 Score','Adjusted R2 Score','Cross Validated R2 Score','RMSE', 'MAE', 'MSE'])
dtScore.insert(0, 'Model', 'Decision Tree')
table = dtScore.to_string(index=False)
print(table)
```


### Random Forest Regressor

```{python}
RandomForest_reg = RandomForestRegressor(n_estimators = 10, random_state = 0)
RandomForest_reg.fit(X_train, y_train)
# Making prediction on data
y_pred = RandomForest_reg.predict(X_test)

ndf = [Regression_metrics(RandomForest_reg, X_train, y_train, X_test, y_test, y_pred)]

rfScore = pd.DataFrame(data = ndf, columns=['R2 Score','Adjusted R2 Score','Cross Validated R2 Score','RMSE', 'MAE', 'MSE'])
rfScore.insert(0, 'Model', 'Random Forest')
table = rfScore.to_string(index=False)
print(table)
```


### KNN Regressor (Nearest Neighbors)

```{python}
KNeighbors_reg = KNeighborsRegressor(n_neighbors=10)
KNeighbors_reg.fit(X_train,y_train)
# Making prediction on data
y_pred = KNeighbors_reg.predict(X_test)

ndf = [Regression_metrics(KNeighbors_reg, X_train, y_train, X_test, y_test, y_pred)]

knnScore = pd.DataFrame(data = ndf, columns=['R2 Score','Adjusted R2 Score','Cross Validated R2 Score','RMSE', 'MAE', 'MSE'])
knnScore.insert(0, 'Model', 'KNN Regressor')
table = knnScore.to_string(index=False)
print(table)
```

### All togheter

```{python}
predictions = pd.concat([dtScore, rfScore, knnScore], ignore_index=True, sort=False)
table = predictions.to_string(index=False)
print(table)
```






# Evaluation




------------------------------------------------------------------------

# Conclusion

Education is a crucial element in our society. The chosen dataset was used to address the prediction of secondary student grades of two core classes (Mathematics and Portuguese) by using past school grades, demographic, social and other school related data. Conducting **Exploratory Data Analysis** is an model_train_mae , model_train_rmse, model_train_r2 = evaluate_model(y_train, y_train_pred)

model_test_mae , model_test_rmse, model_test_r2 = evaluate_model(y_test, y_test_pred)essential step before implementing machine learning models to predict the target attribute in the dataset. EDA (just some attributes for this project) allowed me to understand the structure, quality and underlying patterns of the data, revealing important information about relationships between the attributes.

The project was made with help of the following article: @datasetArticle

# References
