---
title: "Implementation of models - STUDENT PERFORMANCE"
author: "Lucas Jakin"
format: html
editor: visual
---

```{r}
library(tidyverse)
library(dplyr)
library(skimr)
library(ggcorrplot)
library(gt)
library(ggplot2)

dataset <- read.csv("student_data.csv", header = T)
```

```{r}
library(reticulate)
#virtualenv_create("my-pythonImpl", python_version = "3.12")
use_virtualenv("my-pythonImpl", required = TRUE) 

virtualenv_install(envname = "my-pythonImpl", "matplotlib",ignore_installed = FALSE, pip_options = character())

#virtualenv_install(envname = "my-python2", "catboost",ignore_installed = FALSE, pip_options = character())

virtualenv_install(envname = "my-pythonImpl", "numpy",ignore_installed = FALSE, pip_options = character())

virtualenv_install(envname = "my-pythonImpl", "pandas",ignore_installed = FALSE, pip_options = character())

#virtualenv_install(envname = "my-python2", "seaborn",ignore_installed = FALSE, pip_options = character())

virtualenv_install(envname = "my-pythonImpl", "scikit-learn",ignore_installed = FALSE, pip_options = character())

#virtualenv_install(envname = "my-python2", "tensorflow",ignore_installed = FALSE, pip_options = character())
```

## HeatMap of attributes

```{python}
import pandas as pd
import numpy as np  
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error,mean_absolute_error
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import GridSearchCV
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import make_scorer
from tensorflow.keras.models import clone_model
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import KFold
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from scipy import stats

df = pd.read_csv('student_data.csv')

df['AvgGrade'] = (df['G1'] + df['G2'] + df['G3'])/3

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
for column in df.columns:
    if df[column].dtype == 'object':
        df[column] = le.fit_transform(df[column])
        
df.info()

df.to_csv("CorrelationStudent.csv", index=False)
```

### Linear Regression

> Preprocessing

```{r}
datasetTarget <- dataset %>% mutate(TotalGrade = G1 + G2 + G3,
                                      AvgGrade = (G1 + G2 + G3)/ 3)
head(datasetTarget)
write.csv(datasetTarget, "dfAvgGrade.csv", row.names = FALSE)
```

```{python}

student_df = pd.read_csv('dfAvgGrade.csv')

# Categorical attributes
cat_features = [feature for feature in student_df.columns if student_df[feature].dtype == 'O']

# Numerical attributes
num_features = [feature for feature in student_df.columns if student_df[feature].dtype != 'O']

#print(f'We have {len(num_features)} numerical features: {num_features}\n')
#print(f'We have {len(cat_features)} categorical features: {cat_features}')

fig, axs = plt.subplots(1, 2, figsize=(15, 7))
plt.subplot(121)
sns.histplot(data=student_df, x='AvgGrade', bins=30, kde=True, color='g')
plt.subplot(122)
sns.histplot(data=student_df, x='AvgGrade', kde=True, hue='sex')
plt.show()

```

```{python}
X = student_df.drop(['TotalGrade', 'AvgGrade'], axis=1)
y = student_df[['AvgGrade', 'TotalGrade']]

# Transforming the columns 
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(), cat_features)],
    remainder='passthrough'  # Leave the rest of the columns as they are
)

X = preprocessor.fit_transform(X)
```

```{python}
# Function for evaluation
def evaluate_model(true, predicted):
    mae = mean_absolute_error(true, predicted)
    mse = mean_squared_error(true, predicted)
    rmse = np.sqrt(mean_squared_error(true, predicted))
    r2_square = r2_score(true, predicted)
    return mae, rmse, r2_square
```

```{python}
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)

y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)
y_test

```

```{python}
pd.DataFrame({
    'Actual_TotalGrade': y_test['TotalGrade'].values,
    'Predicted_TotalGrade': y_test_pred[:, 1],
    'Actual_AvgGrade': y_test['AvgGrade'].values,
    'Predicted_AvgGrade': y_test_pred[:, 0]
})

```

```         
```

## REGRESSION

```{r}
datasetTarget <- dataset %>% mutate(AvgGrade = (G1 + G2 + G3)/ 3) %>% select(-G1, -G2, -G3)
head(datasetTarget)
datasetTarget <- 
write.csv(datasetTarget, "dfAvgGrade.csv", row.names = FALSE)
```

```{python}
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
student_df = pd.read_csv('student_data.csv')

# Categorical attributes
cat_features = [feature for feature in student_df.columns if student_df[feature].dtype == 'O']

# Numerical attributes
num_features = [feature for feature in student_df.columns if student_df[feature].dtype != 'O']

fig, axs = plt.subplots(1, 2, figsize=(15, 7))
plt.subplot(121)
sns.histplot(data=student_df, x='G3', bins=30, kde=True, color='g')
plt.subplot(122)
sns.histplot(data=student_df, x='G3', kde=True, hue='sex')
plt.show()
```

```{python}

le = LabelEncoder()
for column in student_df.columns:
    if student_df[column].dtype == 'object':
        student_df[column] = le.fit_transform(student_df[column])
        
student_df.info()

X = student_df.drop("G3", axis = 1).values
y = student_df["G3"].values

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=42)

sc = StandardScaler()
X = sc.fit_transform(X_train)
y = sc.fit_transform(y_test.reshape(-1,1))
print(f"X :\n{X}\n")
print(f"y :\n{y}")

print(X.shape)
print(y.shape)
```

> Models & Hyperparameters

```{python}
#| eval: false

models = [
    {
        'name': 'DecisionTreeRegressor',
        'model': DecisionTreeRegressor(),
        'params': {
            'criterion': ['friedman_mse'],
            "max_depth": range(1,20,2),
        }
    },
    {
        'name': 'SVR',
        'model': SVR(),
        'params': {
            "gamma": [0.01],
            'C': [50],
            'kernel': ['rbf'],
        }
    },
    {
        'name': 'RandomForestRegressor',
        'model': RandomForestRegressor(),
        'params': {
            'n_estimators': [300],
            'criterion': ['friedman_mse'],
        }
    },
    {
        'name': 'KNeighborsRegressor',
        'model': KNeighborsRegressor(),
        'params': {
            'n_neighbors': [9],
            'weights': ['distance'],
        }
    },

]



best_models = []
for model_info in models:
    grid_search = GridSearchCV(model_info['model'], model_info['params'], cv=10, n_jobs=-1, scoring = 'accuracy') 
    grid_search.fit(X_train, y_train)  
    best_models.append({
        'name': model_info['name'],
        'best_model': grid_search.best_estimator_,
        'best_params': grid_search.best_params_,
        'best_score': grid_search.best_score_,
    })

for best_model in best_models:
    print(f"Model: {best_model['name']} \nBest Parameters: {best_model['best_params']} \nBest Score: {round(best_model['best_score'], 2)}%")
    print("-" * 30)

model_names = []
model_scores = []
for best_model in best_models:
    score = best_model['best_score']
    if score >= 0:
        model_names.append(best_model['name'])
        model_scores.append(score)

plt.figure(figsize=(12, 6))
plt.bar(model_names, model_scores, color='skyblue')
plt.xlabel('Model')
plt.title('Model Score')
plt.show()

```
